{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "from utils.options import args_parser\n",
    "from utils.utils import exp_details, get_train_test, average_weights, get_model\n",
    "from utils.update import LocalUpdate, test_inference\n",
    "from utils.sampling import dominant_label_sampling, dirichlet_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "\n",
      "Reinforcement Arguments:\n",
      "    Steps Before PPO Update : 256\n",
      "    PPO Learning Rate       : 0.0003\n",
      "    PPO Discount Factor     : 0.9\n",
      "    PPO Batch Size          : 16\n",
      "    PPO Total Timesteps     : 15000\n",
      "    Target Accuracy         : 0.95\n",
      "\n",
      "Federated Arguments:\n",
      "    Number of Users         : 100\n",
      "    Fraction of Users       : 0.1\n",
      "    Local Epochs            : 1\n",
      "    Local Batch Size        : 10\n",
      "    Learning Rate           : 0.001\n",
      "    Momentum                : 0.5\n",
      "    Optimizer               : adam\n",
      "\n",
      "Model Arguments:\n",
      "    Supervision             : True\n",
      "    Architecture            : cnn\n",
      "\n",
      "Misc. Arguments:\n",
      "    Dataset                 : mnist\n",
      "    Number of GPUs          : 1\n",
      "    IID                     : 0\n",
      "    Random Seed             : 1\n",
      "    Test Fraction           : 1\n",
      "    Save Path               : ../../save\n",
      "    Data Path               : ../../data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = args_parser(default=True)\n",
    "args.supervision = True\n",
    "exp_details(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataset, arch, sampling_type, optimal_epochs, client_epochs):\n",
    "    train_dataset, test_dataset = get_train_test(dataset)\n",
    "\n",
    "    base_model = get_model(arch, dataset, \"cuda\")\n",
    "    optimal_model = copy.deepcopy(base_model)\n",
    "    global_model = copy.deepcopy(base_model)\n",
    "    local_models = [copy.deepcopy(base_model) for _ in range(args.num_users)]\n",
    "\n",
    "    if sampling_type == \"dominant_label\":\n",
    "        dict_users = dominant_label_sampling(train_dataset, num_users=args.num_users, num_samples=50_000, gamma=0.8, print_labels=False)\n",
    "    if sampling_type == \"dirichlet\":\n",
    "        dict_users = dirichlet_sampling(train_dataset, num_users=args.num_users, num_samples=30_000, alpha=0.2, print_labels=False)\n",
    "\n",
    "    print(\"Training optimal model...\")\n",
    "    epoch_optimal_params = []\n",
    "    for i in range(optimal_epochs + 1):\n",
    "        # Evaluate accuracy and save parameters\n",
    "        acc, _ = test_inference(supervision=True, device=\"cuda\", model=optimal_model, test_dataset=test_dataset, test_fraction=1)\n",
    "        print(f\"Epoch {i}/{optimal_epochs} | Accuracy of optimal model: {acc}\")\n",
    "        optimal_params = torch.cat([p.flatten() for p in optimal_model.parameters()]).detach().cpu().numpy()\n",
    "        epoch_optimal_params.append(optimal_params)\n",
    "        \n",
    "        if (i == optimal_epochs):\n",
    "            break\n",
    "        \n",
    "        # Perform training\n",
    "        local_update = LocalUpdate(\n",
    "            train_dataset,\n",
    "            range(50_000),\n",
    "            args.local_ep,\n",
    "            args.local_bs,\n",
    "            args.lr,\n",
    "            args.optimizer,\n",
    "            args.supervision,\n",
    "            \"cuda\",\n",
    "        )\n",
    "        local_update.update_weights(optimal_model)\n",
    "    print()\n",
    "    \n",
    "    print(\"Training client fitting models...\")\n",
    "    epoch_client_params = []\n",
    "    for i in tqdm(range(client_epochs)):\n",
    "        \n",
    "        # Save parameters\n",
    "        client_params = []\n",
    "        for i in dict_users.keys():\n",
    "            local_model = local_models[i]\n",
    "            params = torch.cat([p.flatten() for p in local_model.parameters()]).detach().cpu().numpy()\n",
    "            client_params.append(params)\n",
    "\n",
    "        # Train client models\n",
    "        curr_users = np.random.choice(args.num_users, int(args.num_users * args.frac))\n",
    "        for usr in curr_users:\n",
    "            local_model = local_models[usr]\n",
    "            local_update = LocalUpdate(\n",
    "                train_dataset,\n",
    "                dict_users[usr],\n",
    "                args.local_ep,\n",
    "                args.local_bs,\n",
    "                args.lr,\n",
    "                args.optimizer,\n",
    "                args.supervision,\n",
    "                \"cuda\",\n",
    "            )\n",
    "            local_update.update_weights(local_model)\n",
    "\n",
    "        # Aggregate client weights\n",
    "        local_model_weights = [local_models[usr].state_dict() for usr in curr_users]\n",
    "        avg_weights = average_weights(local_model_weights)\n",
    "        global_model.load_state_dict(avg_weights)\n",
    "        for model in local_models:\n",
    "            model.load_state_dict(avg_weights)\n",
    "            \n",
    "        # Save aggregated client params\n",
    "        for i in dict_users.keys():\n",
    "            local_model = local_models[i]\n",
    "            params = torch.cat([p.flatten() for p in local_model.parameters()]).detach().cpu().numpy()\n",
    "            client_params.append(params)\n",
    "            \n",
    "        epoch_client_params.append(client_params)\n",
    "    \n",
    "    return epoch_optimal_params, epoch_client_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(epoch_optimal_params, epoch_client_params, dataset, image_folder):\n",
    "    print(\"Plotting principal components...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(epoch_optimal_params)\n",
    "    pca.fit([params for client_params in epoch_client_params for params in client_params])\n",
    "\n",
    "    epoch_optimal_params_pca = pca.transform(epoch_optimal_params)\n",
    "    epoch_client_params_pca = np.array([pca.transform(client_params) for client_params in epoch_client_params])\n",
    "\n",
    "    all_x = [*epoch_optimal_params_pca[:, 0], *epoch_client_params_pca[:, :, 0].flatten()]\n",
    "    all_y = [*epoch_optimal_params_pca[:, 1], *epoch_client_params_pca[:, :, 1].flatten()]\n",
    "\n",
    "    xlim = (min(all_x), max(all_x))\n",
    "    ylim = (min(all_y), max(all_y))\n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        dataset_name = \"MNIST\"\n",
    "    elif dataset == \"cifar\":\n",
    "        dataset_name = \"CIFAR-10\"\n",
    "\n",
    "    for epoch in tqdm(range(len(epoch_client_params))):\n",
    "        fig, ax = plt.subplots()\n",
    "        pth = ax.scatter(epoch_optimal_params_pca[:, 0], epoch_optimal_params_pca[:, 1], c=range(len(epoch_optimal_params)), cmap=\"plasma\")\n",
    "        fig.colorbar(pth)\n",
    "\n",
    "        ax.scatter(epoch_client_params_pca[epoch, :, 0], epoch_client_params_pca[epoch, :, 1], c=\"g\")\n",
    "        ax.set_title(f\"Principal Components of {dataset_name} Models\")\n",
    "        ax.set_xlabel(\"Principal Component 1\")\n",
    "        ax.set_ylabel(\"Principal Component 2\")\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        \n",
    "        image_path = os.path.join(image_folder, f\"pca_models_{dataset}_epoch_{epoch}.png\")\n",
    "        plt.savefig(image_path)\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(dataset, image_folder, video_path):\n",
    "    print(\"Generating PCA visualization video...\")\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "    images.sort(key=lambda a: int(a.split(\"_\")[-1].split(\".\")[0]))\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(video_path, 0, 1, (width, height))\n",
    "\n",
    "    for image in tqdm(images):\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting principal components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:11<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PCA visualization video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 347.30it/s]\n"
     ]
    }
   ],
   "source": [
    "image_folder = os.path.join(args.save_path, \"pca_visualization/images\")\n",
    "video_path = os.path.join(args.save_path, \"pca_visualization/videos/pca_video_mnist.avi\")\n",
    "\n",
    "epoch_optimal_params, epoch_client_params = train_models(\"mnist\", \"mlp\", \"dominant_label\", 10, 80)\n",
    "\n",
    "plot_pca(epoch_optimal_params, epoch_client_params, \"mnist\", image_folder)\n",
    "make_video(\"mnist\", image_folder, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2021-09-22",
   "language": "python",
   "name": "conda-2021-09-22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
